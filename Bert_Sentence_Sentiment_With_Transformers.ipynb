{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VE8Q6ZOfihOb",
    "outputId": "3408bef7-ae77-4e2f-bc11-9e1443f7630b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.8.1)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "xP_wooOulmg1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "QGOtuOYXnORH"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "4OKE9Ebwo4B4"
   },
   "outputs": [],
   "source": [
    "batch1 = df[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "Gh42rjlexT-s",
    "outputId": "e988dcd4-bb74-4571-b5bf-2c53d0fa674d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a stirring , funny and finally transporting re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>apparently reassembled from the cutting room f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they presume their audience wo n't sit still f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a visually stunning rumination on love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>jonathan parker 's bartleby should have been t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>too bland and fustily tasteful to be truly pru...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>it does n't work as either</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>this one aims for the toilet and scores a dire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>in the name of an allegedly inspiring and easi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>the movie is undone by a filmmaking methodolog...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      0  1\n",
       "0     a stirring , funny and finally transporting re...  1\n",
       "1     apparently reassembled from the cutting room f...  0\n",
       "2     they presume their audience wo n't sit still f...  0\n",
       "3     this is a visually stunning rumination on love...  1\n",
       "4     jonathan parker 's bartleby should have been t...  1\n",
       "...                                                 ... ..\n",
       "1995  too bland and fustily tasteful to be truly pru...  0\n",
       "1996                         it does n't work as either  0\n",
       "1997  this one aims for the toilet and scores a dire...  0\n",
       "1998  in the name of an allegedly inspiring and easi...  0\n",
       "1999  the movie is undone by a filmmaking methodolog...  0\n",
       "\n",
       "[2000 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XCYP7sD_x_6_",
    "outputId": "f25e4758-25c3-4175-ee78-8defe9ffbadb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1041\n",
       "0     959\n",
       "Name: 1, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch1[1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "GF-6JN0Eo41s"
   },
   "outputs": [],
   "source": [
    "# importing pretrained Distilbert and tokenizer\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJ_iDwHzp5Cz",
    "outputId": "1d52a03f-ad1a-4229-a87d-5b7bd004db1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "RiSlNueNqU2m"
   },
   "outputs": [],
   "source": [
    "# Tokenization for the comments\n",
    "\n",
    "tokens = batch1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RRLA1Acur6nE",
    "outputId": "3b380d84-3550-4c4a-f115-f2015237b254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [101, 1037, 18385, 1010, 6057, 1998, 2633, 182...\n",
       "1       [101, 4593, 2128, 27241, 23931, 2013, 1996, 62...\n",
       "2       [101, 2027, 3653, 23545, 2037, 4378, 24185, 10...\n",
       "3       [101, 2023, 2003, 1037, 17453, 14726, 19379, 1...\n",
       "4       [101, 5655, 6262, 1005, 1055, 12075, 2571, 376...\n",
       "                              ...                        \n",
       "1995    [101, 2205, 20857, 1998, 11865, 16643, 2135, 5...\n",
       "1996    [101, 2009, 2515, 1050, 1005, 1056, 2147, 2004...\n",
       "1997    [101, 2023, 2028, 8704, 2005, 1996, 11848, 199...\n",
       "1998    [101, 1999, 1996, 2171, 1997, 2019, 9382, 1898...\n",
       "1999    [101, 1996, 3185, 2003, 25757, 2011, 1037, 244...\n",
       "Name: 0, Length: 2000, dtype: object"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "OJ-2bonisPJd"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokens.values:\n",
    "  if len(i)> max_len:\n",
    "    max_len = len(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1bpnE7KTsQOO",
    "outputId": "e34b6097-5f0e-42d3-ac08-12cb8a9072ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "h4hFIieTsY3L"
   },
   "outputs": [],
   "source": [
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokens.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9sjlNaCwb_D",
    "outputId": "090c41e8-a56c-4744-c928-cc2479917b9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UhulyTifwcxb",
    "outputId": "ff4b4ecb-7075-4c17-cef2-a2709c767021"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 59)"
      ]
     },
     "execution_count": 85,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0,1,0 ) # bert will confuse with the padded values so we need to tell the model to ignore it so we are creating this.\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "QfmGaE4SzQUW"
   },
   "outputs": [],
   "source": [
    "input_id = torch.tensor(padded)\n",
    "attention_mask = torch.tensor(attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "rJ85nkTUzzw9"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "  last_hidden_states = model(input_id, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfO6t5jp0pIm",
    "outputId": "84d44ba1-fa91-4296-be5e-60a2a87b9856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutput([('last_hidden_state',\n",
       "                  tensor([[[-0.2159, -0.1403,  0.0083,  ..., -0.1369,  0.5867,  0.2011],\n",
       "                           [-0.2471,  0.2468,  0.1008,  ..., -0.1631,  0.9349, -0.0715],\n",
       "                           [ 0.0558,  0.3573,  0.4140,  ..., -0.2430,  0.1770, -0.5080],\n",
       "                           ...,\n",
       "                           [-0.0165,  0.1179,  0.3512,  ..., -0.2401,  0.2722, -0.1750],\n",
       "                           [ 0.0961,  0.0667,  0.3147,  ..., -0.3277,  0.3556, -0.2135],\n",
       "                           [ 0.0454,  0.0519,  0.3168,  ..., -0.2880,  0.1844, -0.1042]],\n",
       "                  \n",
       "                          [[-0.1726, -0.1448,  0.0022,  ..., -0.1744,  0.2139,  0.3720],\n",
       "                           [ 0.0022,  0.1684,  0.1269,  ..., -0.1888, -0.0195, -0.0283],\n",
       "                           [ 0.0257, -0.2458,  0.0717,  ..., -0.4339,  0.1622,  0.0133],\n",
       "                           ...,\n",
       "                           [ 0.0505, -0.0493,  0.0463,  ..., -0.0448, -0.0540,  0.3136],\n",
       "                           [-0.2128, -0.1907, -0.0215,  ...,  0.0139, -0.2433, -0.0202],\n",
       "                           [-0.1310, -0.1693,  0.1019,  ..., -0.0859, -0.1770, -0.0872]],\n",
       "                  \n",
       "                          [[-0.0506,  0.0720, -0.0296,  ..., -0.0715,  0.7185,  0.2623],\n",
       "                           [ 0.0536,  0.3136, -0.0598,  ...,  0.2676,  0.8668, -0.3380],\n",
       "                           [ 0.3792,  0.2792,  0.0237,  ...,  0.0343,  0.4272,  0.1680],\n",
       "                           ...,\n",
       "                           [ 0.2314,  0.2427,  0.1030,  ..., -0.0840,  0.2322,  0.0613],\n",
       "                           [ 0.0741,  0.0465,  0.1083,  ...,  0.0530,  0.1971,  0.0233],\n",
       "                           [ 0.1042,  0.3139,  0.0768,  ..., -0.0301,  0.1052, -0.0780]],\n",
       "                  \n",
       "                          ...,\n",
       "                  \n",
       "                          [[-0.2783, -0.2480,  0.1359,  ..., -0.1904,  0.1310,  0.3498],\n",
       "                           [-0.4406, -0.5159,  0.1700,  ..., -0.3430,  0.3747,  0.0846],\n",
       "                           [-0.4604, -0.3928,  0.0390,  ..., -0.3068, -0.1666,  0.2803],\n",
       "                           ...,\n",
       "                           [-0.2033, -0.2213,  0.2879,  ..., -0.2334, -0.1342,  0.1061],\n",
       "                           [-0.0998, -0.2407,  0.1394,  ..., -0.1828, -0.0762,  0.2031],\n",
       "                           [-0.0663, -0.1063,  0.1606,  ..., -0.2004, -0.2571,  0.1438]],\n",
       "                  \n",
       "                          [[-0.0367,  0.1064, -0.0111,  ..., -0.1121,  0.4162,  0.5034],\n",
       "                           [-0.1869,  0.3485, -0.8375,  ..., -0.1169,  0.3985,  0.3065],\n",
       "                           [-0.3108,  0.2174, -0.1678,  ..., -0.1267,  0.3512, -0.0812],\n",
       "                           ...,\n",
       "                           [ 0.0874,  0.0203,  0.0381,  ...,  0.1545,  0.0125,  0.1109],\n",
       "                           [ 0.2528, -0.0149,  0.0847,  ...,  0.0441,  0.1973,  0.2889],\n",
       "                           [ 0.2378, -0.0568,  0.2862,  ...,  0.0842,  0.1540,  0.1655]],\n",
       "                  \n",
       "                          [[ 0.1240,  0.0143,  0.0104,  ..., -0.1161,  0.5346,  0.2750],\n",
       "                           [ 0.1294, -0.2348, -0.0882,  ...,  0.0997,  1.1743, -0.3309],\n",
       "                           [ 0.5874, -0.1074, -0.0561,  ..., -0.0401,  0.3627, -0.4122],\n",
       "                           ...,\n",
       "                           [ 0.4066, -0.1452,  0.0651,  ...,  0.0879,  0.3980, -0.1256],\n",
       "                           [ 0.4385, -0.1488,  0.1139,  ..., -0.0744,  0.3416, -0.3278],\n",
       "                           [ 0.5451, -0.0851,  0.1311,  ..., -0.0017,  0.3413, -0.3513]]]))])"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "47mYmd3i2Ssm"
   },
   "outputs": [],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QNaTafmO1_hx",
    "outputId": "6d066309-c0d7-430c-d350-a323244f4991"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21593425, -0.14028913,  0.00831123, ..., -0.13694875,\n",
       "         0.5867002 ,  0.20112707],\n",
       "       [-0.1726271 , -0.14476179,  0.0022342 , ..., -0.17442557,\n",
       "         0.21386456,  0.37197497],\n",
       "       [-0.05063327,  0.07203942, -0.02959672, ..., -0.07148942,\n",
       "         0.71852356,  0.26225507],\n",
       "       ...,\n",
       "       [-0.27829766, -0.24803594,  0.13585812, ..., -0.19039148,\n",
       "         0.13099591,  0.3497837 ],\n",
       "       [-0.03667733,  0.10638569, -0.01111017, ..., -0.1120664 ,\n",
       "         0.41619503,  0.50338006],\n",
       "       [ 0.12402605,  0.01425171,  0.01038414, ..., -0.11606554,\n",
       "         0.5345917 ,  0.2749535 ]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "ciVFj1yG2U7P"
   },
   "outputs": [],
   "source": [
    "labels = batch1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NdTYQkml2clk",
    "outputId": "cf254f82-35d2-4e69-aa15-239ea4f39083"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "1995    0\n",
       "1996    0\n",
       "1997    0\n",
       "1998    0\n",
       "1999    0\n",
       "Name: 1, Length: 2000, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "CmZdtB4T2dng"
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(features, labels, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "VOP-aRyn5EHV"
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LFRAfKB_5Ovc",
    "outputId": "7a19c835-0aca-4b69-feaf-2d417cd104cf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "xiEXXRhS5WqL"
   },
   "outputs": [],
   "source": [
    "y_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JaHgbmJP5mXB",
    "outputId": "eb4d2f0a-7f42-404f-859e-f6943948130d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 0, 1])"
      ]
     },
     "execution_count": 97,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOAXkcSb5npb",
    "outputId": "171dda9c-d437-4aa4-8e4e-b2d40e54815d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "3EHTK1hi5_8-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_score = accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jpjp-uxi6LEC",
    "outputId": "3e084d89-53f7-4ca2-a914-8b34b865de93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 100,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "CjzUWzHO6L_R"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQP4GVJg6aNy",
    "outputId": "4fa7ecf6-2738-46f2-ebbe-694f681b97fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[158,  40],\n",
       "       [ 32, 170]])"
      ]
     },
     "execution_count": 102,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "4ygvPK8o6axn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Bert_Sentence_Sentiment_With_Transformers.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
